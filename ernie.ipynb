{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GermEval2021_Ernie.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnCnPrpoV-hl"
      },
      "source": [
        "from ipywidgets import interact, widgets\n",
        "\n",
        "# Input Subtask\n",
        "print(\"1: Toxic Comment Classification\\n2: Engaging Comment Classification\\n3: Fact Claiming Comment Classification\\nChoose Subtask:\")\n",
        "TASK_NUMBER = int(input())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS8j7-2z1EIH"
      },
      "source": [
        "!pip install -q ernie\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, confusion_matrix, plot_roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from ernie import SentenceClassifier, Models, clean_autosave\n",
        "\n",
        "def evaluation_result(true, predicted):\n",
        "  print(f\"Accuracy Score: {round(accuracy_score(true,predicted) * 100,2)} %\\n\")\n",
        "  # accuracy\n",
        "  accuracy = accuracy_score(true,predicted)\n",
        "  print('Accuracy: %f' % accuracy)\n",
        "  # macro f1 \n",
        "  f1 = f1_score(true,predicted, average='macro')\n",
        "  print('F1 score: %f' % f1)\n",
        "  # ROC AUC\n",
        "  auc = roc_auc_score(true,predicted)\n",
        "  print('ROC AUC: %f' % auc)\n",
        "  # precision\n",
        "  precision = precision_score(true,predicted)\n",
        "  print('Precision: %f' % precision)\n",
        "  # recall\n",
        "  recall = recall_score(true,predicted)\n",
        "  print('Recall: %f' % recall)\n",
        "  # confusion matrix\n",
        "  matrix = confusion_matrix(true,predicted)\n",
        "  print(matrix)\n",
        "  # classification report\n",
        "  report = classification_report(true,predicted)\n",
        "  print(report)\n",
        "  print(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h3md6NoMkGT"
      },
      "source": [
        "# load training dataset\n",
        "df = pd.read_csv('GermEval21_Translated_final.csv')\n",
        "df.columns\n",
        "df['English_Google'] = df['English_Google'].fillna(df['comment_text'])\n",
        "df['English_Google'] = df['English_Google'].apply(lambda x: np.str_(x))\n",
        "\n",
        "# load test dataset\n",
        "test_df = pd.read_csv('GermEval21_Test_Translated.csv')\n",
        "test_df.columns\n",
        "test_df['English_Google'] = test_df['English_Google'].fillna(test_df['c_text'])\n",
        "test_df['English_Google'] = test_df['English_Google'].apply(lambda x: np.str_(x))\n",
        "\n",
        "# load final test labels\n",
        "label_df = pd.read_csv('truth.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMGChbGANo18"
      },
      "source": [
        "# Split training dataset\n",
        "if TASK_NUMBER == 1:\n",
        "    df_new = df.filter([\"English_Google\", \"Sub1_Toxic\"])\n",
        "    df_new = df_new.rename(columns={'English_Google' : '0'})\n",
        "    df_new = df_new.rename(columns={'Sub1_Toxic' : '1'})\n",
        "    trainn, testn = train_test_split(df_new, test_size=0.1, random_state=101, stratify=df_new['1'])\n",
        "    df_ger = df.filter([\"comment_text\", \"Sub1_Toxic\"])\n",
        "    df_ger = df_ger.rename(columns={'comment_text' : '0'})\n",
        "    df_ger = df_ger.rename(columns={'Sub1_Toxic' : '1'})\n",
        "    traing, testg = train_test_split(df_ger, test_size=0.1, random_state=101, stratify=df_ger['1'])\n",
        "elif TASK_NUMBER == 2:\n",
        "    df_new = df.filter([\"English_Google\", \"Sub2_Engaging\"])\n",
        "    df_new = df_new.rename(columns={'English_Google' : '0'})\n",
        "    df_new = df_new.rename(columns={'Sub2_Engaging' : '1'})\n",
        "    trainn, testn = train_test_split(df_new, test_size=0.1, random_state=101, stratify=df_new['1'])\n",
        "    df_ger = df.filter([\"comment_text\", \"Sub2_Engaging\"])\n",
        "    df_ger = df_ger.rename(columns={'comment_text' : '0'})\n",
        "    df_ger = df_ger.rename(columns={'Sub2_Engaging' : '1'})\n",
        "    traing, testg = train_test_split(df_ger, test_size=0.1, random_state=101, stratify=df_ger['1'])\n",
        "elif TASK_NUMBER == 3:\n",
        "    df_new = df.filter([\"English_Google\", \"Sub3_FactClaiming\"])\n",
        "    df_new = df_new.rename(columns={'English_Google' : '0'})\n",
        "    df_new = df_new.rename(columns={'Sub3_FactClaiming' : '1'})\n",
        "    trainn, testn = train_test_split(df_new, test_size=0.1, random_state=101, stratify=df_new['1'])\n",
        "    df_ger = df.filter([\"comment_text\", \"Sub3_FactClaiming\"])\n",
        "    df_ger = df_ger.rename(columns={'comment_text' : '0'})\n",
        "    df_ger = df_ger.rename(columns={'Sub3_FactClaiming' : '1'})\n",
        "    traing, testg = train_test_split(df_ger, test_size=0.1, random_state=101, stratify=df_ger['1'])\n",
        "\n",
        "# rename truth dataset\n",
        "if TASK_NUMBER == 1:\n",
        "    label_df = label_df.filter([\"Sub1_Toxic\"])\n",
        "    label_df = label_df.rename(columns={'Sub1_Toxic' : '1'})\n",
        "elif TASK_NUMBER == 2:\n",
        "    label_df = label_df.filter([\"Sub2_Engaging\"])\n",
        "    label_df = label_df.rename(columns={'Sub2_Engaging' : '1'})\n",
        "elif TASK_NUMBER == 3:\n",
        "    label_df = label_df.filter([\"Sub3_FactClaiming\"])\n",
        "    label_df = label_df.rename(columns={'Sub3_FactClaiming' : '1'})\n",
        "    \n",
        "# BERT Base Uncased (epochs=3)\n",
        "print('='*50)\n",
        "print('BERT Base Uncased (epochs=3)') \n",
        "print('-'*50)\n",
        "print(\" \")\n",
        "classifier = SentenceClassifier(model_name=Models.BertBaseUncased, max_length=128)\n",
        "classifier.load_dataset(trainn, validation_split=0.1, stratify=trainn['1'])\n",
        "classifier.fine_tune(epochs=3)\n",
        "bertb_eval = [np.round(classifier.predict_one(each), decimals=0) for each in testn['0']]\n",
        "true = testn['1'].tolist()\n",
        "lst = []\n",
        "for arr in bertb_eval:\n",
        "    lst.append(np.argmax(arr))\n",
        "predicted = lst\n",
        "evaluation_result(true, predicted)\n",
        "bertb = [np.round(classifier.predict_one(each), decimals=0) for each in test_df['English_Google']]\n",
        "true = label_df['1'].tolist()\n",
        "lst = []\n",
        "for arr in bertb:\n",
        "    lst.append(np.argmax(arr))\n",
        "predicted = lst\n",
        "evaluation_result(true, predicted)\n",
        "clean_autosave()\n",
        "print(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJl3Wjthegk7"
      },
      "source": [
        "# mBERT Base Cased (epochs=3)\n",
        "print('='*50)\n",
        "print('mBERT Base Cased (epochs=3)') \n",
        "print('-'*50)\n",
        "print(\" \")\n",
        "classifier = SentenceClassifier(model_name='bert-base-multilingual-cased', max_length=128)\n",
        "classifier.load_dataset(trainn, validation_split=0.1, stratify=trainn['1'])\n",
        "classifier.fine_tune(epochs=3)\n",
        "mbertc_eval = [np.round(classifier.predict_one(each), decimals=0) for each in testn['0']]\n",
        "true = testn['1'].tolist()\n",
        "lst = []\n",
        "for arr in mbertc_eval:\n",
        "    lst.append(np.argmax(arr))\n",
        "predicted = lst\n",
        "evaluation_result(true, predicted)\n",
        "mbertc = [np.round(classifier.predict_one(each), decimals=0) for each in test_df['English_Google']]\n",
        "true = label_df['1'].tolist()\n",
        "lst = []\n",
        "for arr in mbertc:\n",
        "    lst.append(np.argmax(arr))\n",
        "predicted = lst\n",
        "evaluation_result(true, predicted)\n",
        "clean_autosave()\n",
        "print(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KAkXAQpeciF"
      },
      "source": [
        "# DBMDZ GermanBERT Base Cased (epochs=3)\n",
        "print('='*50)\n",
        "print('DBMDZ GermanBERT Base Cased (epochs=3)') \n",
        "print('-'*50)\n",
        "print(\" \")\n",
        "classifier = SentenceClassifier(model_name='dbmdz/bert-base-german-cased', max_length=128)\n",
        "classifier.load_dataset(traing, validation_split=0.1, stratify=trainn['1'])\n",
        "classifier.fine_tune(epochs=3)\n",
        "germanbert_eval = [np.round(classifier.predict_one(each), decimals=0) for each in testg['0']]\n",
        "true = testn['1'].tolist()\n",
        "lst = []\n",
        "for arr in germanbert_eval:\n",
        "    lst.append(np.argmax(arr))\n",
        "predicted = lst\n",
        "evaluation_result(true, predicted)\n",
        "germanbert = [np.round(classifier.predict_one(each), decimals=0) for each in test_df['c_text']]\n",
        "true = label_df['1'].tolist()\n",
        "lst = []\n",
        "for arr in germanbert:\n",
        "    lst.append(np.argmax(arr))\n",
        "predicted = lst\n",
        "evaluation_result(true, predicted)\n",
        "clean_autosave()\n",
        "print(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh1mfzW6eaMH"
      },
      "source": [
        "# Deepset GermanBERT Base Cased (epochs=3)\n",
        "print('='*50)\n",
        "print('Deepset GermanBERT Base Cased (epochs=3)') \n",
        "print('-'*50)\n",
        "print(\" \")\n",
        "classifier = SentenceClassifier(model_name='bert-base-german-cased', max_length=128)\n",
        "classifier.load_dataset(traing, validation_split=0.1, stratify=trainn['1'])\n",
        "classifier.fine_tune(epochs=3)\n",
        "rlgermanbert_eval = [np.round(classifier.predict_one(each), decimals=0) for each in testg['0']]\n",
        "true = testn['1'].tolist()\n",
        "lst = []\n",
        "for arr in rlgermanbert_eval:\n",
        "    lst.append(np.argmax(arr))\n",
        "predicted = lst\n",
        "evaluation_result(true, predicted)\n",
        "rlgermanbert = [np.round(classifier.predict_one(each), decimals=0) for each in test_df['c_text']]\n",
        "true = label_df['1'].tolist()\n",
        "lst = []\n",
        "for arr in rlgermanbert:\n",
        "    lst.append(np.argmax(arr))\n",
        "predicted = lst\n",
        "evaluation_result(true, predicted)\n",
        "clean_autosave()\n",
        "print(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1QgX6qoeWt0"
      },
      "source": [
        "# mBERT Base Cased (epochs=3)\n",
        "print('='*50)\n",
        "print('mBERT Base Cased (epochs=3)') \n",
        "print('-'*50)\n",
        "print(\" \")\n",
        "classifier = SentenceClassifier(model_name='bert-base-multilingual-cased', max_length=128)\n",
        "classifier.load_dataset(traing, validation_split=0.1, stratify=trainn['1'])\n",
        "classifier.fine_tune(epochs=3)\n",
        "mbertcg_eval = [np.round(classifier.predict_one(each), decimals=0) for each in testg['0']]\n",
        "true = testn['1'].tolist()\n",
        "lst = []\n",
        "for arr in mbertcg_eval:\n",
        "    lst.append(np.argmax(arr))\n",
        "predicted = lst\n",
        "evaluation_result(true, predicted)\n",
        "mbertcg = [np.round(classifier.predict_one(each), decimals=0) for each in test_df['c_text']]\n",
        "true = label_df['1'].tolist()\n",
        "lst = []\n",
        "for arr in mbertcg:\n",
        "    lst.append(np.argmax(arr))\n",
        "predicted = lst\n",
        "evaluation_result(true, predicted)\n",
        "clean_autosave()\n",
        "print(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS1PLCUIrGWi"
      },
      "source": [
        "bertb_r = []\n",
        "mbertc_r = []\n",
        "germanbert_r = []\n",
        "rlgermanbert_r = []\n",
        "mbertcg_r = []\n",
        "for each in bertb:\n",
        "  bertb_r.append(int(each[1]))\n",
        "for each in mbertc:\n",
        "  mbertc_r.append(int(each[1]))\n",
        "for each in germanbert:\n",
        "  germanbert_r.append(int(each[1]))\n",
        "for each in rlgermanbert:\n",
        "  rlgermanbert_r.append(int(each[1]))\n",
        "for each in mbertcg:\n",
        "  mbertcg_r.append(int(each[1]))\n",
        "\n",
        "data = pd.DataFrame([test_df['comment_id'].tolist(), bertb_r, mbertc_r, germanbert_r, rlgermanbert_r, mbertcg_r]) #Each list would be added as a row\n",
        "data = data.transpose() #To Transpose and make each rows as columns\n",
        "data.columns=['comment_id', 'bertb', 'mbertc', 'germanbert', 'rlgermanbert', 'mbertcg'] #Rename the columns\n",
        "\n",
        "if TASK_NUMBER == 1:\n",
        "  data.to_csv('task1_E.csv', index=False)\n",
        "elif TASK_NUMBER == 2:\n",
        "  data.to_csv('task2_E.csv', index=False)\n",
        "elif TASK_NUMBER == 3:\n",
        "  data.to_csv('task3_E.csv', index=False)\n",
        "\n",
        "display(data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}